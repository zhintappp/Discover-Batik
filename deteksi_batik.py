# -*- coding: utf-8 -*-
"""Deteksi Batik.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14wmVR4e-UC8z_nlyVbP7wpnDJLP--evu
"""

from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.backend import clear_session
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.models import load_model
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import os
import opendatasets as od

od.download('https://www.kaggle.com/datasets/alfanme/indonesian-batik-motifs-corak-app?select=DATASET')

train_path = '/content/indonesian-batik-motifs-corak-app/DATASET/DATASET/TRAIN'
test_path = '/content/indonesian-batik-motifs-corak-app/DATASET/DATASET/TEST'

train_datagen = ImageDataGenerator(
  #  rescale=1./255,
    shear_range=0.2,
    rotation_range=30,
    fill_mode='reflect',
    zoom_range=0.2,
    brightness_range=[0.8, 1.2],
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    vertical_flip=True
)

validation_datagen = ImageDataGenerator(
    #rescale=1./255
)

train_generator = train_datagen.flow_from_directory(
    directory=train_path,
    target_size=(224,224),
    class_mode='categorical',
    batch_size=30,
    color_mode='rgb',
)

validation_generator = validation_datagen.flow_from_directory(
    directory=test_path,
    target_size=(224,224),
    class_mode='categorical',
    batch_size=30,
    color_mode='rgb',
)

motives_dict = train_generator.class_indices
print(motives_dict)

base_model = EfficientNetB0(
    include_top=False,
    weights='imagenet',
    input_shape=(224, 224, 3),
#     alpha=1.0,
    pooling='avg'
)


base_model.trainable = False

clear_session()

model = Sequential([
    base_model,
    Dense(64, activation='relu'),
    Dropout(0.25),
    Dense(15, activation='softmax')
])


model.compile(
    optimizer=Adam(learning_rate=1e-4), #0.0001
    loss='categorical_crossentropy',
    metrics=['accuracy'],
)

model.summary()

history = model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=50,
    shuffle=True,
)

train_acc = history.history['accuracy']
train_loss = history.history['loss']
val_acc = history.history['val_accuracy']
val_loss = history.history['val_loss']

plt.figure(figsize=(16,6))
plt.subplot(1, 2, 1)
plt.title('Accuracy Curve')
plt.plot(train_acc, label='Training')
plt.plot(val_acc, label='Validation')
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('Accuracy')

plt.subplot(1, 2, 2)
plt.title('Loss Curve')
plt.plot(train_loss, label='Training')
plt.plot(val_loss, label='Validation')
plt.legend()
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.show()

test_dir = '/content/indonesian-batik-motifs-corak-app/DATASET/DATASET/TEST'

for motive in motives_dict.keys():
    images_list = os.listdir(os.path.join(test_dir, motive))
#     plt.figure(figsize=(16, 20))

    correct_count = 0

    for idx, image in enumerate(images_list):
        img_path = f'/content/indonesian-batik-motifs-corak-app/DATASET/DATASET/TEST/{motive}/{image}'
        img = load_img(img_path, target_size=(224, 224))

        img_array = img_to_array(img)
        img_array = tf.expand_dims(img_array, 0)

        motives_list = list(motives_dict.keys())
        prediction = model(img_array)
        pred_idx = np.argmax(prediction)
        pred_motive = motives_list[pred_idx]
        pred_confidence = prediction[0][pred_idx] * 100

        if pred_motive == motive:
            correct_count += 1

#         plt.subplot(5, 4, idx+1)
#         plt.title(f'Actual: {motive}\nPrediction: {pred_motive} {pred_confidence:.2f}%')
#         plt.imshow(img)
#         plt.axis('off')

    print(f'{motive} Predictions --> {correct_count}/20 correct')
# plt.show()